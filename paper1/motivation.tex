\label{sec:motivation}

Bug-finding tools have been an active area of research for almost as long as computer programs have existed. 
Techniques such as abstract interpretation, fuzzing, and symbolic execution with constraint solving have been proposed, developed. and applied.
But evaluation has been a problem, as  ground truth is in extremely short supply.
Vulnerability corpora exist [cite SAMATE] but they are of limited utility and quantity.
These corpora fall into two categories: historic and synthetic.
Corpora built from historic vulnerabilities such as [cite ours and others] contain too few examples to be of much use.
These are closest to what we would want to have since the bug is embedded in real code and are often well annotated with precise information about where the bug manifests itself.
The author's own experience creating such a corpus was that it is a difficult and lengthy process; a corpus of only fourteen very well annotated historic bugs with triggering inputs took about six months to construct. 
In addition, public corpora have the disadvantage of already being released, and so we can expect tools to have been trained to detect those bugs.
Given the price tag of new exploitable bugs, which is widely understood to begin in the mid five figures [cite Hacking Team, http://www.wired.com/2015/07/hacking-team-leak-shows-secretive-zero-day-exploit-sales-work/], it is hard to find bugs for our corpus that have not been explicitly used to train existing bug-finding tools.
And, while synthetic code stocked with bugs, auto-generated by scripts, can provide large numbers of diverse and diagnostic examples, each is only a tiny program and the constructions are often considered oddball and unrepresentative of real code~\cite{kendra}.

In practice, a vulnerability discovery tool is typically evaluated by running it and seeing what it finds. 
Thus, one technique is judged superior if it finds more bugs than another.
While this state of affairs is perfectly understandable, given the scarcity of ground truth, it is an obstacle to science and progress in vulnerability discovery.
There is currently no way to measure fundamental figures of merit such as miss and false alarm rate for a bug finding tool.

We propose the following requirements for bugs in a vulnerability corpus, if it is to be useful for research, development, and evaluation.
Bugs must be
\begin{enumerate}
\item Cheap and plentiful
\item Span the execution lifetime of a program
\item Embedded in representative control and data flow
\item Come with an input that serves as an existence proof 
\item Manifest for a very small fraction of possible inputs
\end {enumerate}
The first requirement, if we can meet it, is highly desirable since it enables frequent evaluation and hill climbing. 
Corpora are more valuable if they are essentially disposable. 
The second and third of these requirements stipulate that bugs must be realistic.
The fourth means the bug is real and serious, and is a precondition for determining exploitability. 
The fifth is crucial.
Consider the converse: if a bug manifests for all or a large fraction of inputs it is trivially discoverable by simply running the program.

The approach we propose is to create a synthetic vulnerability via a few judicious and automated edits to the source code of a real program.
We will detail and give results for an implementation of this approach that satisfies all of the above requirements.
We call this implementation LAVA for Lincoln Application Vulnerability Assessment.    
A serious bug such as a buffer overflow can be injected by LAVA into a program like \verb+file+, which is 13K LOC, in about a 15 seconds.
LAVA bugs manifest all along the execution trace, in all parts of the program, shallow and deep, and make use of mostly completely normal data flow.
By construction, a LAVA bug comes with an input that triggers it, and no other input can have this effect upon the program.
