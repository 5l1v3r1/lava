\label{sec:motivation}

Bug-finding tools have been an active area of research for almost as long as computer programs have existed. 
Techniques such as abstract interpretation, fuzzing, and symbolic execution with constraint solving have been proposed, developed, and applied.
But evaluation has been a problem, as  ground truth is in extremely short supply.
Vulnerability corpora exist~\cite{Kass:2005} but they are of limited utility and quantity.
These corpora fall into two categories: historic and synthetic.
Corpora built from historic vulnerabilities contain too few examples to be of much use~\cite{Zitser:2004}.
However, these are closest to what we want to have since the bugs are embedded in real code, use real inputs, and are often well annotated with precise information about where the bug manifests itself.

Creating such a corpus is a difficult and lengthy process; according to the authors of prior work on bug-finding tool evaluation, a corpus of fourteen very well annotated historic bugs with triggering inputs took about six months to construct~\cite{ActuallyTim}.
In addition, public corpora have the disadvantage of already being released, and thus rapidly become stale; as we can expect tools to have been trained to detect bugs that have been released.
Given the commercial price tag of new exploitable bugs, which is widely understood to begin in the mid five figures~\cite{Tsyrklevich:2015}, it is hard to find real bugs for our corpus that have not already been used to train tools.
And while synthetic code stocked with bugs auto-generated by scripts can provide large numbers of diagnostic examples, each is only a tiny program and the constructions are often considered unrepresentative of real code~\cite{Juliet:2012,Kratkiewicz:2005}.

In practice, a vulnerability discovery tool is typically evaluated by running it and seeing what it finds. 
Thus, one technique is judged superior if it finds more bugs than another.
While this state of affairs is perfectly understandable, given the scarcity of ground truth, it is an obstacle to science and progress in vulnerability discovery.
There is currently no way to measure fundamental figures of merit such as miss and false alarm rate for a bug finding tool.

We propose the following requirements for bugs in a vulnerability corpus, if it is to be useful for research, development, and evaluation.
Bugs must

\begin{enumerate}
\item Be cheap and plentiful
\item Span the execution lifetime of a program
\item Be embedded in representative control and data flow
\item Come with an input that serves as an existence proof 
\item Manifest for a very small fraction of possible inputs
\end {enumerate}

\noindent
The first requirement, if we can meet it, is highly desirable since it enables frequent evaluation and hill climbing.
Corpora are more valuable if they are essentially disposable. 
The second and third of these requirements stipulate that bugs must be realistic.
The fourth means the bug is demonstrable and serious, and is a precondition for determining exploitability. 
The fifth requirement is crucial.
Consider the converse: if a bug manifests for a large fraction of inputs it is trivially discoverable by simply running the program.

The approach we propose is to create a synthetic vulnerability via a few judicious and automated edits to the source code of a real program.
We will detail and give results for an implementation of this approach that satisfies all of the above requirements, which we call LAVA (Large-scale Automated Vulnerability Addition).
A serious bug such as a buffer overflow can be injected by LAVA into a program like \verb+file+, which is 13K LOC, in about 15 seconds.
LAVA bugs manifest all along the execution trace, in all parts of the program, shallow and deep, and make use of normal data flow.
By construction, a LAVA bug comes with an input that triggers it, and we can guarantee that no other input can trigger the bug.
