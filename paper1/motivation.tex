Bug-finding tools have been an active area of research for as long as computer programs have existed. Techniques like abstract interpretation and fuzzing, along with newer approaches like concolic execution, claim to be able to find exploitable vulnerabilities in real programs.  But it has long been difficult to evaluate these techniques' comparative utility. Effectively, bug-finding tools have been trained on the only test data we have, which is existing bugs. Acquiring a large corpus of previously unreleased bugs is prohibitively expensive, as exploitable bugs can sell for thousands of dollars on the open market, and no one has really been interested in collecting the non-exploitable ones.

The only realistic option is to create synthetic bugs in programs.
With an on-demand corpus of synthetic bugs, we can guarantee a few key properties.
First of all, we can ensure that the bugs have not been seen before?in particular, that the developer of the bug-finding tool did not consider that particular bug during the creation of their tool.
We can know exactly where the bug is located, as well as details about actuation and dataflow from the input.
We can also be certain to have a large enough corpus that we can compute meaningful statistics on the results of the bug-finding tool.