\label{section:results}

\begin{table*}[t]
\caption{LAVA Injection results for open source programs of various sizes}
\centering\footnotesize
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}
        &         & Num       & Lines     &        &        & Potential & Validated  &         & Inj Time \\
Name    & Version & Src Files & C code    & N(DUA) & N(ATP) & Bugs      & Bugs       & Yield   & (sec)  \\\hline
file    & 5.22    & 19        & 10809     & 631    & 114    & 17518     & 774        & 38.7\%  & 16         \\  %    files verified.  sloc recomputed
%eog    & 3.4.2   &           & 22997     &        &        &           &            &         &         \\ 
readelf & 2.25    & 12        & 21052     & 3849   & 266    & 276367    & 1064       & 53.2 \% & 354     \\  % time verified.  files verified. sloc recomputed
bash    & 4.3     & 143       & 98871     & 3832   & 604    & 447645    & 192        & 9.6\%   & 153     \\  % time verified.  
tshark  & 1.8.2   & 1272      & 2186252   & 9853   & 1037   & 1240777   & 354        & 17.7\%  & 542     \\
\end{tabular} 
%For each, a single input file was used to perform a taint analysis with PANDA.
%Various program and dynamic trace statistics are reported as well as DUA, attack point (ATP), and yield (fraction of injected bugs that result in a segmentation violation).}
\label{table:insertion-results}
\end{table*}

We evaluated LAVA in three ways.
First, we injected large numbers of bugs into four open source programs: file, readelf (from binutils), bash, and tshark (the command-line version of the packet capture and analysis tool Wireshark).
For each of these, we report various statistics with respect to both the target program and also LAVA's success at injecting bugs.
Second, we evaluated the distribution and realism of LAVA's bugs by proposing and computing various measures.
Finally, we performed a preliminary investigation to see how effective existing bug-finding tools are at finding LAVA's bugs, by measuring the detection rates of an open-source fuzzer and a symbolic execution-based bug finder.

\subsection*{Counting Bugs}

Before we delve into the results, we must specify what it is we mean by an injected bug, and what makes two injected bugs distinct. Although there are many possible ways to define a bug, we choose a definition that best fits our target use case: two bugs should be considered different if an automated tool would have to reason about them differently. For our purposes, we define a bug as a unique pair $(DUA, attack point)$. Expanding this out, that means that the source file, line number, and variable name of the DUA, and the source file and line number of the attack point must be unique.

Some might object that this artificially inflates the count of bugs injected into the program, for example because it would consider two bugs distinct if they differ in where the file input becomes available to the program, even though the same file input bytes are used in both cases. 
But in fact these should be counted as different bugs: the data and control flow leading up to the point where the DUA occurs will be very different, and vulnerability discovery tools will have to reason differently about the two cases.

\subsection{Injection Experiments}
\label{sec:results:subsec:injection}

The results of injecting bugs into open source programs are summarized in Table~\ref{table:insertion-results}.
In this table, programs are ordered by size, in lines of C code, as measured by David Wheeler's \verb+sloccount+.
A single input was used with each program to measure taint and find injectable bugs.
The input to \verb+file+ and \verb+readelf+ was the program \verb+ls+.
The input to \verb+tshark+ was a 16K packet capture file from a site hosting a number of such examples.  % [ http://www.stearns.org/toolscd/current/pcapfile/README.ethereal-pcap.html ]
%The input to \verb+eog+ was ... 
The input to \verb+bash+ was a 124-line shell script written by the authors.
% TRL -- removed this since table was too big and I wanted to add bug inj time
%The number of sequential and unique basic blocks in the PANDA trace for each program are reported as N(BBs) and N(BBu).
$N(DUA)$ and $N(ATP)$ are the number of DUAs and attack points collected by the \verb+FIB+ analysis.
Note that, in order for a DUA or attack point to be counted, it must have been deemed viable for some bug, as described in Section~\ref{sec:mining}.
The columns \emph{Potential Bugs} and \emph{Validated Bugs} in Table~\ref{table:insertion-results} give the numbers of both potential bugs found by \verb+FIB+, but also those verified to actually return exitcodes indicating a buffer overflow (-11 for segfault or -6 for heap corruption) when run against the modified input.
The penultimate column in the table is \emph{Yield} which is the fraction of potential bugs what were tested and determined to be actual buffer overflows.
The last column gives the time required to test a single potential bug injection for the target.


Exhaustive testing was not possible for a number of reasons.
Larger targets had larger numbers of potential bugs and take longer to test; for example, \verb+tshark+ has over a million bugs and took almost 10 minutes to test.
This is because testing requires not only injecting a small amount of code to add the bug, but also recompiling and running the resulting program.
For many targets, we found the build to be subtly broken so that a \verb+make clean+ was necessary to pick up the bug injection reliably, which further increased testing time.
Instead, we attempted to validate 2000 potential bugs chosen uniformly at random for each target.
Thus, when we report, in Table~\ref{table:insertion-results} that for \verb+tshark+, the yield is 17.7\%, this is because 306 out of 2000 bugs were found to be valid.

As the injected bug is designed to be triggered only if a particular set of four bytes in the input is set to a magic value, we tested with both the original input and with the modified one that contained the trigger. 
We did not encounter any situation in which the original input triggered a crash.

\begin{table}[b]
\caption{Yield as a function of both $mLIV$ and $mTCN$}
\centering\footnotesize
\begin{tabular}{l|l|l|l|l} 
       & \multicolumn{3}{c}{$mLIV$} &  \\  
$mTCN$ &         $[0,10)$ & $[10,100)$ & $[100,1000)$ & $[1000,+\inf]$ \\  \hline 
$[0,10)$ &       51.9\%   & 22.9\%     & 17.4\%       & 11.9\%          \\
$[10,100)$ &     --       & 0          & 0            & 0     \\
$[100,+\inf]$ &  --       & --         & --           & 0     \\ 
\end{tabular}
%Yield is highest for DUAs with low values for both of these measures, i.e., that are both a relatively uncomplicated function of input bytes and also that derive from input bytes involved in deciding fewer branches.
%Cells for which there were no samples are indicated with the contents '--'.}
\label{table:yield-breakdown}
\end{table}

Yield varies considerably from less than 10\% to over 50\%.
To understand this better, we investigated the relationship between our two taint-based measures and yield.
For each DUA used to inject a bug, we determined $mTCN$, the maximum TCN for any of its bytes and $mLIV$, the maximum liveness for any label in any taint label set associated with one of its bytes.  
More informally, $mTCN$ represents how complicated a function of the input bytes a DUA is, and $mLIV$ is a measure of how much the control flow of a program is influenced by the input bytes that determine a DUA.
Table~\ref{table:yield-breakdown} is a two-dimensional histogram with the bins for $mTCN$ intervals along the vertical axis and bins for $mLIV$ along the horizontal axis.
The top-left cell of this table represents all bug injections for which $mTCN<10$ and $mLIV<10$, and the bottom-right cell is all those for which $mTCN>=1000$ and $mLIV>=1000$.
Recall that when  $mTCN=mLIV=0$, the DUA is not only a direct copy of input bytes, but those input bytes have also not been observed to be used in deciding any program branches. 
As either $mTCN$ or $mLIV$ increase, yield deteriorates.  
However, we were surprised to observe that $mLIV$ values of over 1000 still gave yield in the 10\% range.

\subsection{Bug Distribution}

It would appear that LAVA can inject a very large number of bugs into a program.
If we extrapolate from yield numbers in Table~\ref{table:insertion-results}, we estimate there would be almost 400,000 real bugs if all were tested.
But how well distributed is this set of bugs? 
For programs like \verb+file+ and \verb+bash+, between 11 and 44 source files  are involved in a potential bug.
In this case, the bugs appear to be fairly well distributed, as those numbers represent 58\% and 31\% of the total for each, respectively.
On the other hand, \verb+readelf+ and \verb+tshark+ fare worse, with only 2 and 122 source files found to involve a potential bug for each (16.7\% and 9.6\% of source files).
For \verb+tshark+, much of the code for which is devoted to parsing esoteric network protocols, coverage is probably the issue since we use only a single input.
Similiary, we only use a single hand-written script with \verb+bash+, with little attempt to cover a majority of language features.
We are unsure why so few of the source files in \verb+readelf+ involve a potential bug.
 

\subsection{Bug Realism}

%\todo[inline]{Ricky: I think it would be particularly convincing to have a "case study" of an actual CVE that exhibits data flow properties similar to the kind of bug LAVA injects.  Or maybe a bug that adds unchecked tainted data to a pointer.} 

\begin{figure}
\centering
\includegraphics[width=3in]{trace-dua-atp.png}
\caption{A cartoon representing an entire program trace, annotated with instruction count at which DUA is siphoned off to be used, $I(DUA)$, attack point where it is used, $I(ATP)$, and total number of instructions in trace, $I(TOT)$.}
\label{fig:dua-atp-trace}
\end{figure}

The intended use of the bugs created by this system is as ground truth for development and evaluation of vulnerability discovery tools and techniques. 
Thus, it is crucial that they be realistic in some sense.  
Realism is, however, difficult to assess.

Because this work is, to our knowledge, the first to consider the problem of fully automated bug injection, we are not able to make use of any standard measures for bug realism.
Instead, we devised our own measures, focusing on features such as how well distributed the malformed data input and trigger points were in the program's execution, as well as how much of the original behavior of the program was preserved.

We examined three aspects of our injected bugs as measures of realism. 
The first two are DUA and attack point position within the program trace, which are depicted in Figure~\ref{fig:dua-atp-trace}.
That is, we determined the fraction of trace instructions executed at the point the DUA is siphoned off and at the point it is used to attack the program by corrupting an internal program value.

Histograms for these two quantities, $I(DUA)$ and $I(ATP)$, are provided in Figures~\ref{fig:dua-hist} and~\ref{fig:atp-hist}, where counts are for all potential bugs in the LAVA database for all five open source programs. 
DUAs and attack points are clearly available at all points during the trace, although there appear to be more at the beginning and end.
This is important, since bugs created using these DUAs have entirely realistic control and data-flow all the way up to $I(DUA)$.
Therefore, vulnerability discovery tools will have to reason correctly about all of the program up to $I(DUA)$ in order to correctly diagnose the bug.

Our third metric concerns the portion of the trace \emph{between} the $I(DUA)$ and $I(ATP)$.
This segment is of particular interest since LAVA currently makes data flow between DUA and attack point via a pair of function calls.
Thus, it might be argued that this is an unrealistic portion of the trace in terms of data flow.
The quantity $I(DUA)/I(ATP)$ will be close to 1 for injected bugs that minimize this source of unrealism.
This would correspond to the worked example in Figure~\ref{fig:worked-example}; the DUA is still in scope when, a few lines later in the same function, it can be used to corrupt a pointer.
No abnormal data flow is required.
The histogram in Figure~\ref{fig:rdf-hist} quantifies this effect for all potential LAVA bugs, and it is clear that a large fraction have $I(DUA)/I(ATP) \approx 1$, and are therefore highly realistic by this metric.

\begin{figure}
\centering
\includegraphics[width=3in]{dua.pdf}
\caption{Normalized DUA trace location}
\label{fig:dua-hist}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=3in]{atp.pdf}
\caption{Normalized ATP trace location}
\label{fig:atp-hist}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=3in]{rdf.pdf}
\caption{Fraction of trace with perfectly normal or realistic data flow, $I(DUA)/I(ATP)$}
\label{fig:rdf-hist}
\end{figure}




\subsection{Vulnerability Discovery Tool Evaluation}

We ran two vulnerability discovery tools on LAVA-injected bugs to investigate their use in evaluation.

\begin{enumerate}
\item Coverage guided fuzzer (referred to as FUZZER)
\item Symbolic execution + SAT solving (referred to as SES)
\end{enumerate}

These two, specifically, were chosen because fuzzing and symbolic execution are extremely popular techniques for finding real-world bugs.
FUZZER and SES are both state-of-the-art, high-profile tools. 
For each tool, we expended significant effort to ensure that we were using them correctly.
This means carefully reading all documentation, blog posts, and email lists.
Additionally, we constructed tiny example buggy programs and used them to verify that we were able to use each tool at least to find known easy bugs.  
For some tools we had to resort to reading source code in order to determine the proper command-line incantations to employ. 
We did not contact tool authors for guidance in tool use, as this seemed likely to bias results.

Note that the names of tools under evaluation are being withheld in reporting results.
Careful evaluation is a large and important job, and we would not want to give it short shrift, either in terms of careful setup and use of tools, or in presenting and discussing results.
Our intent, here, is to determine if LAVA bugs \emph{can be used} to evaluate bug finding systems. 
It is our expectation that, in future work either by ourselves or others, full and careful evaluation of real, named tools will be performed using LAVA.
While that work is outside the scope of this paper, we hope to indicate that it should be both possible and valuable. 
Additionally, it is our plan and hope that LAVA bugs will be made available in quantity and at regular refresh intervals, for self-evaluation and hill climbing.

The first corpus we created, \emph{LAVA-1}, used the \verb+file+ target, the smallest of those programs into which we have injected bugs.
This corpus consists of sixty-nine buffer overflow bugs injected into the source with LAVA, each on a different branch in a \verb+git+ repository with a fuzzed version of the input verified to trigger a crash checked in along with the code.
Two types of buffer overflows were injected, each of which makes use of a single 4-byte DUA to trigger and control the overflow.

\begin{enumerate}
    \item \textbf{Knob-and-trigger}. 
In this type of bug, two bytes of the DUA (the \emph{trigger}) are used to test against a magic value to determine if the overflow will happen.
The other two bytes of the DUA (the \emph{knob}) determine how much to overflow. 
Thus, these bugs manifest if a 2-byte unsigned integer in the input is a particular value but only if another 2-bytes in the input are big enough to cause trouble. 
    \item \textbf{Range}. 
These bugs trigger if the magic value is simply in some range, but also use the magic value to determine how much to overflow.
The magic value is a 4-byte unsigned integer and the range varies.  
\end{enumerate}

These bug types were designed to mirror real bugs patterns.  
In knob-and-trigger bugs, two different parts of the input are used in different ways to determine the manifestation of the bug.  
In range bugs, rather than triggering on a single value out of $2^{32}$, the size of the haystack varies.
Note that a range of $2^0$ is equivalent to the bug presented in Figure~\ref{src:dua-use}.

\begin{table}[h]
\caption{Percentage of bugs found in \emph{LAVA-1} corpus} %. % as a function of $Bug Type$ and $Tool Type$.  
\centering\footnotesize
\begin{tabular}{l|l|l|l|l|l|l} 
Tool   &                     \multicolumn{6}{|c}{Bug Type}                           \\  \hline  
         &                     \multicolumn{5}{|c|}{Range}                   &     \\   
         &    $2^0$   & $2^7$       & $2^{14}$     & $2^{21}$   & $2^{28}$     & KT   \\  \hline 
FUZZER &    0       & 0           & 0            & 7\%        & 58\%         & 0         \\
%$CSA_1$ &    0       & 0           & 0            & 0          & 0            & 0         \\
SES    &    8\%     & 0           & 9\%          & 21\%       & 0            & 10\%         \\
\end{tabular}
%\caption{Percentage of bugs found in \emph{LAVA-1} corpus. % as a function of $Bug Type$ and $Tool Type$.  
%FUZZER proved to be an effective vulnerability finding tool.
%However, it was only able to find bugs that allowed an enormous range of possible inputs to trigger the bug.}
\label{table:eval1-file}
\end{table}

The results of this evaluation are summarized in Table~\ref{table:eval1-file}.
Five different ranges were employed: $2^0, 2^7, 2^{16}, 2^{21}, 2^{28}$. 
We examined all output from both tools.
FUZZER ran for one hour on each bug and found 7\% of those with a range of $2^{21}$ and 58\% of those with a range size of $2^{28}$.
SES ran for five hours on each bug, and found several bugs in all categories except the $2^7$ and $2^{28}$ ranges.

The results for the \emph{LAVA-1} corpus seem to accord well with how these tools work.
FUZZER uses the program largely as a black box, randomizing individual bytes, and guiding exploration with coverage measurements.
Bugs that trigger if and only if a four-byte extent in the input is set to a magic value are unlikely to be discovered in this way.
Given time, FUZZER finds bugs that trigger for ranges of a million or more bytes. 
Note that for many of these LAVA bugs, when the range is so large, discovery is possible by simply fuzzing every byte in the input a few times.  
These bugs may, in fact, be trivially discoverable with a regression suite for a program like \verb+file+ that accepts arbitrary file input\footnote{In principle, anyway. In practice \texttt{file}'s test suite consists of just 3 tests, none of which trigger our injected bugs.}.
By contrast, SES is able to find both knob-and-trigger bugs and different ranges, and the size of the range does not affect the number of bugs found.
This is because it is no more difficult for a SAT solver to find a satisfying input for a large range or a small range; rather, the number of bugs found is limited by how deep into the program the symbolic execution can go.

Note that having each bug in a separate copy of the program means that for each run of a bug finding tool, only one bug is available for discovery at a time.  
This is one kind of evaluation, but it seems to disadvantage tools like FUZZER and SES, which appear to be designed to work for a long time (10s of hours) on a single program that may contain multiple bugs. 

Thus, we created a second corpus, \emph{LAVA-M}, in which we injected more than one bug at a time into the source code.
We chose four programs from the \verb+coreutils+ suite that took file input: \verb+base64+, \verb+md5sum+, \verb+uniq+, and \verb+who+.
Into each, we injected as many verified bugs as possible.
Because the \verb+coreutils+ programs are quite small, and because we only used a single input file for each to perform the taint analysis, the total number of bugs injected into each program was generally quite small.
The one exception to this pattern was the \verb+who+ program, which parses a binary file with many dead or even unused fields, and therefore had many DUAs available for bug injection.
We were not able to inject multiple bugs of the two types described above (knob-and-trigger and range), as interactions became a problem, and so all bugs were of the type in Figure~\ref{src:dua-use}, which trigger for only a single setting of four input bytes.  
The \emph{LAVA-M} corpus, therefore, is four copies of the source code for \verb+coreutils+ version 8.24.
One copy has 44 bugs injected into \verb+base64+, and comes with 44 inputs known to trigger those bugs individually.
Another copy has 57 bugs in \verb+md5sum+, another has 28 bugs in \verb+uniq+.
Finally, there is a copy of \verb+coreutils+ with 2136 bugs existing all at once and individually expressible, in \verb+who+.


\begin{table}[h]
\caption{Bugs found in \emph{LAVA-M} corpus by tool type}
\centering\footnotesize
\begin{tabular}{l|c|c|c|c} 
\multirow{2}{*}{Tool Name} & \multirow{2}{*}{Total Bugs} & \multicolumn{3}{c}{Unique Bugs Found} \\
              &            & FUZZER       & SES        & Combined \\ \hline 
\verb+uniq+   &    28      & 7            & 0          & 7               \\
\verb+base64+ &    44      & 7            & 9          & 14               \\
\verb+md5sum+ &    57      & 2            & 0          & 2               \\
\verb+who+    &    2136    & 0            & 18         & 18               \\
Total         &    2265    & 16           & 27         & 41               \\
\end{tabular}
\label{table:tool-eval-results-coreutils}
\end{table}

We ran FUZZER and SES against each program in \emph{LAVA-M}, with 5 hours of runtime for each program on machines with essentially unlimited RAM.
\verb+md5sum+ ran with the \verb+-c+ argument, as if it were checking digests in a symbolic file.
\verb+base64+ ran with the \verb+-d+ argument, to decode base 64.

SES found no bugs in \verb+uniq+ or \verb+md5sum+.
In \verb+uniq+, we believe this is because the control flow is too unconstrained.
In \verb+md5sum+, SES failed to execute any code past the first instance of the hash function.
\verb+base64+ and \verb+who+ both turn out more successful for SES.
The tool finds 9 bugs in \verb+base64+ out of 44 inserted; these include both deep and shallow bugs, as base64 is such a simple program to analyze.

SES's results are a little more complicated for \verb+who+.
All of the bugs it finds for \verb+who+ use one of two DUAs, and all of them occur very early in the trace.
One artifact of our method for injecting multiple bugs simultaneously is that multiple bugs share the same attack point.
It is debatable how realistically this aspect represents real bugs.
In practice, it means that SES can only find one bug per attack point, as finding an additional bug at the same ATP does not necessarily require covering new code.
LAVA could certainly be changed to have each bug involve new code coverage.
SES could also be improved to find all the bugs at each attack point, which means generating multiple satisfying inputs for the same set of conditions.

FUZZER found bugs in all utilities except \verb+who+\footnote{In fact, we allowed FUZZER to continue running after 5 hours had passed; it managed to find a bug in \texttt{who} in the sixth hour.}.
Unlike SES, the bugs were fairly uniformly distributed throughout the program, as they depend only on guessing the correct 4-byte trigger at the right position in the input file.

FUZZER's failure to find bugs in \verb+who+ is surprising.
We speculate that the size of the seed file (the first 768 bytes of a \verb+utmp+ file) used for the fuzzer may have been too large to effectively explore through random mutation, but more investigation is necessary to pin down the true cause.
Indeed, tool anomalies of this sort are exactly the sort of thing one would hope to find with LAVA, as they represent areas where tools might make easy gains.

We note that the bugs found by FUZZER and SES have very little overlap (only 2 bugs where found by both utility).
This is a very promising result for LAVA, as it indicates that the kinds of bugs created by LAVA are not tailored to a particular bug finding strategy.

%We also evalued a second commercial static analyzer, referred to in this text as $CSA_2$. Because we were only allowed a limited number of analysis runs, we opted to create a single version of \verb+who+ from the GNU coreutils that contained 60 injected, verified bugs. This was fed to $CSA_2$, generating 117 alerts; we then went through its alerts by hand to see which ones referenced our injected bugs.

%Of the 117 alerts, 17 referred to code we had injected into \verb+who+. We did not investigate the other 100 further; presumably these are some mix of true and false positives in the original source code to \verb+who+. The 17 that were specific to our injected bugs were examined in more detail. Fourteen were found to be essentially false positives: they referred to innocuous artifacts of the injection process. For example, to guard against introducing pointer errors, we emit code like \verb+if(p) { ... }+; however, the static analysis software took this as proof that \verb+p+ might be NULL, and then warned us that it was dereferenced elsewhere before the NULL check. After discounting such artifacts, we found that $CSA_2$ correctly identified three of our injected bugs, labeling them all as ``Out-of-bounds access''.
