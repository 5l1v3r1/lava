\label{section:results}

\begin{table*}[t!]
\centering
\begin{tabular}{c|c|c|c|c|c|c|c|c|c} 
        &         & Lines    &        &        & Potential & Validated  &         & Inj Time \\
Name    & Version & C code   & N(DUA) & N(ATP) & Bugs      & Bugs       & Yield   & (sec)  \\\hline
file    & 5.22    & 12927    & 631    & 114    & 17518     & 774        & 38.7\%  & 14         \\
%eog    & 3.4.2   & 22997    &        &        &           &            &         &         \\ 
bash    & 4.3     & 98871    & 3832   & 604    & 447645    & 192        & 9.6\%   & 281     \\
readelf & 2.25    & 1327513  & 3849   & 266    & 276367    & 1064       & 53.2 \% & 344     \\
tshark  & 1.8.2   & 2186252  & 9853   & 1037   & 1240777   & 354        & 17.7\%  & 542     \\
\end{tabular} 
\caption{LAVA Injection results for open source programs of various sizes.
For each, a single input file was used to perform a taint analysis with PANDA.
Various program and dynamic trace statistics are reported as well as DUA, attack point (ATP), and yield (fraction of injected bugs that result in a segmentation violation).}
\label{table:insertion-results}
\end{table*}

%We evaluated LAVA in three ways.
We evaluated LAVA in two ways.
First, we injected large numbers of bugs into five open source programs: file, readelf (from binutils), bash, and tshark (command-line verison of Wireshark). %, and eog (aka Eye of GNOME, an image viewer).
For each of these, we report various statistics with respect to both the target program and also LAVA's success at injecting bugs.
Second, we evaluated the realism of LAVA's bugs by proposing and computing some measures.
%Third, we randomly sampled 20 injected bugs for each of these programs and used these to measure detection rates for two commercial and two open source bug finders.

\subsection{Injection Experiments}

The results of injecting bugs into open source programs are summarized in Table~\ref{table:insertion-results}.
In this table, programs are ordered by size, in lines of C code, as measured by David Wheeler's \verb+sloccount+. 
A single input was used with each program to measure taint and find injectable bugs.
The input to \verb+file+ and \verb+readelf+ was the program \verb+ls+.
The input to \verb+tshark+ was a 16K packet capture file from a site hosting a number of such examples.  % [ http://www.stearns.org/toolscd/current/pcapfile/README.ethereal-pcap.html ]
%The input to \verb+eog+ was ... 
The input to \verb+bash+ was a 124-line shell script written by the authors.
% TRL -- removed this since table was too big and I wanted to add bug inj time
%The number of sequential and unique basic blocks in the PANDA trace for each program are reported as N(BBs) and N(BBu).
$N(DUA)$ and $N(ATP)$ are the number of DUAs and attack points collected in the LAVA database by the FIB analysis.
Note that, in order for a DUA or attack point to be counted, it must have been deemed viable for some bug, as described in Section~\ref{sec:mining}.
The columns \emph{Potential Bugs} and \emph{Validated Bugs} in Table~\ref{table:insertion-results} give the numbers of both potential bugs found by FIB, but also those verified to actually return exitcodes indicating a buffer overflow (-11 for segfault or -6 for heap corruption) when run against the modified input.
The penultimate column in the table is \emph{Yield} which is the fraction of potential bugs what were validated to be actual buffer overflows.   
The last column gives the time required to test a single potential bug injection for the target.


Exhaustive testing was not possible; for each of the target programs, we attempted to validate 2000 potential bugs. 
Note that larger targets had larger numbers of potential bugs: \verb+tshark+ has over a million.
Larger targets also take longer to test: almost 10 minutes for \verb+tshark+.
This is because testing requires not only injecting a small amount of code to add the bug, but also recompiling and running the resulting program.
For many targets, we found the build to be subtly broken so that a \verb+make clean+ was necessary to pick up the bug injection reliably, which further increased testing time.

As the injected bug is designed to be triggered only if a particular set of four bytes in the input is set to a magic value, we tested with both the original input and with the modified one that contained the trigger. 
We did not encounter any situation in which the original input caused an overflow.


Yield varies considerably from a 9.6\% to over 50\%.
To understand this better, we investigated the relationship between our two taint-based measures and yield.
For each DUA used to inject a bug, we determined $mTCN$, the maximum TCN for any of its bytes and $mLIV$, the maximum liveness for any label in any taint set associated with one of its bytes.  
More informally, $mTCN$ represents how complicated a function of the input bytes a DUA is, and $mLIV$ is a measure of how much the control flow of a program is influenced by the input bytes that determine a DUA.
Table~\ref{table:yield-breakdown} is a two-dimensional histogram with the bins for $mTCN$ intervals along the vertical axis and bins for $mLIV$ along the horizontal axis.
The top-left cell of this table represents all bug injections for which $mTCN<10$ and $mLIV<10$, and the bottom-right cell is all those for which $mTCN>=1000$ and $mLIV>=1000$.
Recall that, when  $mTCN=mLIV=0$, the DUA is not only a direct copy of input bytes, but also, those input bytes have not been observed to be used in deciding any program branches. 
As either $mTCN$ or $mLIV$ increase, yield deteriorates.  

\begin{table}
\centering
\begin{tabular}{l|l|l|l|l} 
       & \multicolumn{3}{c}{$mLIV$} &  \\  
$mTCN$ &         $[0,10)$ & $[10,100)$ & $[100,1000)$ & $[1000,+\inf]$ \\  \hline 
$[0,10)$ &       0.519    & 0.229      & 0.174        & 0.119          \\
$[10,100)$ &     --       & 0          & 0            & 0     \\
$[100,+\inf]$ &  --       & --         & --           & 0     \\ 
\end{tabular}
\caption{Yield as a function of both $mLIV$ and $mTCN$.  
Yield is highest for DUAs with low values for both of these measures, i.e., that are both an uncomplicated function of input bytes and also that do not derive from input bytes that are used to decide program control flow.
Cells for which there were no samples are indicated with the contents '--'.}
\label{table:yield-breakdown}
\end{table}

\subsection{Bug Diversity}
It would appear as though LAVA can inject a very large number of bugs.  
If we extrapolate from yield numbers in Table~\ref{table:insertion-results}, we estimate there are over 400,000 validated bugs.
  But how diverse is this set of bugs?

\subsection{Bug Realism}

\begin{figure}
\centering
\includegraphics[width=3in]{trace-dua-atp.png}
\caption{A cartoon representing an entire program trace, annotated with instruction count at which DUA is siphoned off to be used, $I(DUA)$, attack point where it is used, $I(ATP$), and total number of instructions in trace, $I(TOT)$.}
\label{fig:dua-atp-trace}
\end{figure}

The intended use of the bugs created by this system is as ground truth for development and evaluation of vulnerability discovery tools and techniques. 
Thus, it is crucial that they be realistic in some sense.  
Realism is, however, difficult to assess.
We examined three aspects of our injected bugs as measures of realism. 
The first two are DUA and attack point position within the program trace, which are depicted in Figure~\ref{fig:dua-atp-trace}.
That is, we determined the fraction of trace instructions executed at the point the DUA is siphoned off and at the point it is used to attack the program by corrupting a pointer.
Histograms for these two quantities, $I(DUA)$ and $I(ATP)$, are provided in Figures~\ref{fig:dua-hist} and~\ref{fig:atp-hist}, where counts are for all potential bugs in the LAVA database for all five open source programs. 
DUAs and attack points are clearly available at all points during the trace, although there appear to be more at the beginning and end.
This is important, since bugs created using these DUAs have entirely realistic control and data-flow all the way up to $I(DUA)$.
Therefore, vulnerability discovery tools will have to reason correctly about all of the program up to $I(DUA)$ in order to correctly diagnose the bug.
The portion of the trace \emph{between} the $I(DUA)$ and $I(ATP)$ is of particular interest since, currently, LAVA makes data flow between DUA and attack point via a pair of function calls.
Thus, it might be argued that this is an unrealistic portion of the trace in terms of data flow.
The quantity $I(DUA)/I(ATP)$ will be close to 1 for injected bugs that minimize this source of unrealism.
This would correspond to the worked example in Figure~\ref{fig:worked-example}; the DUA is still in scope, when, a few lines later in the same function, it can be used to corrupt a pointer.
No abnormal data flow is required.
The histogram in Figure~\ref{fig:rdf-hist} quantifies this effect for all potential LAVA bugs, and it is clear that a large fraction have $I(DUA)/I(ATP) \approx 1$, and are therefore highly realistic.

\begin{figure}
\centering
\includegraphics[width=3in]{dua.pdf}
\caption{Normalized DUA trace location}
\label{fig:dua-hist}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=3in]{atp.pdf}
\caption{Normalized DUA trace location}
\label{fig:atp-hist}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=3in]{rdf.pdf}
\caption{Fraction of trace with perfectly normal or realistic data flow, $I(DUA)/I(ATP)$}
\label{fig:rdf-hist}
\end{figure}




\subsection{Vulnerability Discovery Tool Evaluation}

And something needs to go here when we get it from NEU who is doing the eval.



